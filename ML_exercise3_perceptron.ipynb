{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyOW1175+0+pUQgfpm3aMzdp",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/RogueRock/IDC410-ML/blob/main/ML_exercise3_perceptron.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QaZ7b77vKsrP",
        "outputId": "f02f5899-3b72-4147-8ce3-1ac79d1f774f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/gdrive; to attempt to forcibly remount, call drive.mount(\"/content/gdrive\", force_remount=True).\n"
          ]
        }
      ],
      "source": [
        "#ML exercise 3\n",
        "#To implement A Multi-layered Perceptron Using Numpy\n",
        "#linking gdrive with colab so as to access the input file\n",
        "from google.colab import drive\n",
        "drive.mount('/content/gdrive')"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Q1.\n",
        "#importing pandas reading the dataframe from the input file\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "#copying the file path\n",
        "file_path = \"/content/gdrive/MyDrive/mnist_train.csv\"\n",
        "df=pd.read_csv(file_path)"
      ],
      "metadata": {
        "id": "7joO7CHkGeas"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#checking the size and shape of the input file\n",
        "size = df.size\n",
        "shape = df.shape\n",
        "print (size, shape)\n",
        "#using iloc to slice the df and look at first_ten columns and first 200 rows\n",
        "first_ten = df.iloc[0:200, :10]\n",
        "print (first_ten)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4QbtDge1ICO7",
        "outputId": "f3ba3399-1623-4558-eff7-970fc76008c0"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "47100000 (60000, 785)\n",
            "     label  1x1  1x2  1x3  1x4  1x5  1x6  1x7  1x8  1x9\n",
            "0        5    0    0    0    0    0    0    0    0    0\n",
            "1        0    0    0    0    0    0    0    0    0    0\n",
            "2        4    0    0    0    0    0    0    0    0    0\n",
            "3        1    0    0    0    0    0    0    0    0    0\n",
            "4        9    0    0    0    0    0    0    0    0    0\n",
            "..     ...  ...  ...  ...  ...  ...  ...  ...  ...  ...\n",
            "195      9    0    0    0    0    0    0    0    0    0\n",
            "196      7    0    0    0    0    0    0    0    0    0\n",
            "197      8    0    0    0    0    0    0    0    0    0\n",
            "198      3    0    0    0    0    0    0    0    0    0\n",
            "199      2    0    0    0    0    0    0    0    0    0\n",
            "\n",
            "[200 rows x 10 columns]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#function to take file path as input\n",
        "def function_1 (file_path):\n",
        "  df = pd.read_csv(file_path)\n",
        "  arr1 = df.to_numpy(dtype = float) #converting to a numppy array\n",
        "  x = arr1[:,1:] #removing first class cloumn\n",
        "  arr2 = df.to_numpy(dtype = int)\n",
        "  y = arr2[:, :1] #slicing to get the desired data\n",
        "  y2 = np.transpose(y) #to get a single row\n",
        "  return y2, x\n",
        "y, x  = function_1 (\"/content/gdrive/MyDrive/mnist_train.csv\")\n",
        "print (y[:,:10])\n",
        "print (x.shape, y.shape)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OCX6Wgv-OCpA",
        "outputId": "cb0174e9-ce64-4314-8777-d7bfd8449350"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[[5 0 4 1 9 2 1 3 1 4]]\n",
            "(60000, 784) (1, 60000)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Q2\n",
        "#making a class for the softmax activation function\n",
        "class SoftmaxReg():\n",
        "#initializing the instance\n",
        "  def __init__(self):\n",
        "    self.weight = np.random.randn(784, 10)/10\n",
        "    self.bias = np.random.rand(1, 10)/10\n",
        "   # self.softmax = softmax\n",
        "    print (self.bias.shape)\n",
        "#doing the forward pass and then applying the softmax function\n",
        "  def forward_prop(self, x):\n",
        "    self.x = x\n",
        "    self.z = np.matmul(x, self.weight) + self.bias\n",
        "    self.sigmas = self.softmax(self.z)\n",
        "    print(\"Shape of sigmas:\", self.sigmas.shape)\n",
        "    return self.sigmas\n",
        "#doing the back propagation using the cross entropy loss function derivative\n",
        "#Reference ; https://jmlb.github.io/ml/2017/12/26/Calculate_Gradient_Softmax/\n",
        "  def backprop(self,x, y, learn_rate):\n",
        "    #self.target = y - np.log(self.sigmas)\n",
        "    #loss = np.sum(-self.target)\n",
        "    self.derivative1 = np.matmul(x.T, (self.sigmas - y.T))/60000\n",
        "    self.derivative2 = np.mean(self.sigmas - y.T, axis=0, keepdims=True)\n",
        "    self.weight = self.weight - (learn_rate*self.derivative1)/60000\n",
        "    self.bias = self.bias + (learn_rate*self.derivative2)/60000\n",
        "    return self.weight, self.bias\n",
        "  def softmax(self,z):\n",
        "    max_z = np.max(self.z, axis=1, keepdims=True)\n",
        "    #<ipython-input-28-9f6efb757d7b>:23: RuntimeWarning: overflow encountered in exp\n",
        "    #therefore using the max to subtract from logits of z, log-sum-exp approach\n",
        "    #Reference : https://gregorygundersen.com/blog/2020/02/09/log-sum-exp/\n",
        "    #e = np.exp(self.z)\n",
        "    e = np.exp(self.z - max_z)\n",
        "    e_sum = np.sum(e, axis = 1, keepdims = True)\n",
        "    sigma = e/e_sum\n",
        "    return sigma\n",
        "\n",
        "model1 = SoftmaxReg()\n",
        "learn_rate = 0.001\n",
        "# forward prop\n",
        "sigmas = model1.forward_prop(x)\n",
        "\n",
        "# back prop\n",
        "updated_weights, updated_bias = model1.backprop(x, y, learn_rate)\n",
        "\n",
        "# printing for debugging\n",
        "print(\"Updated weights shape:\", updated_weights.shape)\n",
        "print(\"updated bias shape:\", updated_bias.shape )"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KIQg4fSegI5X",
        "outputId": "d929ca07-9b60-4c15-8956-c19743753c6a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(1, 10)\n",
            "Shape of sigmas: (60000, 10)\n",
            "Updated weights shape: (784, 10)\n",
            "updated bias shape: (1, 10)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Q3.\n",
        "#making a class for adding just one hidden layer of 16 neurons\n",
        "class HiddenLayer1 ():\n",
        "\n",
        "  def __init__ (self, x, softmax):\n",
        "    self.x = x\n",
        "    self.softmax = softmax\n",
        "    #self.sigmoid = sigmoid\n",
        "  #def generator (self):\n",
        "    self.weight1 = np.random.randn(784, 16)\n",
        "    self.bias1 = np.random.rand(1, 16)\n",
        "    self.weight2 = np.random.randn(16, 10)\n",
        "    self.bias2 = np.random.rand(1, 10)\n",
        "    self.activation_2 = self.sigmoid\n",
        "    self.activation_3 = self.softmax\n",
        "\n",
        "  '''def layer_size (self, x, hidden_shape, output_shape):\n",
        "    self.inputshape = self.x.shape[0]\n",
        "    self.hlshape = hidden_shape\n",
        "    self.out_shape = output_shape'''\n",
        "\n",
        "  def forward_prop(self, x):\n",
        "    self.x = x\n",
        "    self.z1 = np.dot(self.x, self.weight1) + self.bias1\n",
        "    self.y_hat1 = self.activation_2(self.z1)\n",
        "    self.z2 = np.dot(self.y_hat1, self.weight2) + self.bias2\n",
        "    self.y_hat2 = self.activation_3(self.z2) #self.softmax(self.z2)\n",
        "    print(\"Shape of y_hat1:\", self.y_hat1.shape)\n",
        "    return self.y_hat2\n",
        "\n",
        "  def back_prop(self, y, learn_rate):\n",
        "    dz1 = (self.y_hat2 - y.T)\n",
        "    #gradient for weights 1\n",
        "    self.deri_weight2 = np.sum(np.matmul(self.z2.T, dz1))/60000\n",
        "    #dividing for normalization\n",
        "    self.deri_bias2 = np.mean(self.y_hat2 - y.T, axis=0, keepdims=True)\n",
        "    #gradient for weights 1\n",
        "    self.deri_weight1 = np.sum(np.matmul(self.x.T, dz1))/60000\n",
        "    #self.deri_bias1 = np.mean(self.y_hat2 - y.T, axis=0, keepdims=True)\n",
        "    #updating weights\n",
        "    self.weight1 = self.weight1 - (learn_rate*self.deri_weight1)/60000\n",
        "    #self.bias1 = self.bias1 - (learn_rate*self.deri_bias1)/60000\n",
        "    self.weight2 = self.weight2 - (learn_rate*self.deri_weight2.T)/60000\n",
        "    #self.bias2 = self.bias1 - (learn_rate*self.deri_bias2)/60000\n",
        "    return self.weight1, self.weight2, #self.bias1, self.bias2\n",
        "\n",
        "  def sigmoid (self, z):\n",
        "    z = np.dot(self.x, self.weight1) + self.bias1\n",
        "    sigma = 1 / (np.exp(z) + 1)\n",
        "    return sigma\n",
        "  #def sigmoid_deri (self, z):\n",
        "   # return self.x * (1 - self.x)\n",
        "\n",
        "model2 = HiddenLayer1(x, model1.softmax)\n",
        "learn_rate = 0.001\n",
        "#forward prop\n",
        "y_hat2 = model2.forward_prop(x)\n",
        "\n",
        "# back prop\n",
        "updated_weights1, weight2 = model2.back_prop(y, learn_rate)\n",
        "\n",
        "# printing for debugging\n",
        "print(\"Updated weights shape:\", updated_weights1.shape, weight2.shape)\n",
        "#print(\"updated bias shape:\", bias1.shape, bias2.shape )"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Tv0Utv4kak56",
        "outputId": "3307338d-8159-4230-9cf5-724fab6898ab"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-110-340b3e954109>:50: RuntimeWarning: overflow encountered in exp\n",
            "  sigma = 1 / (np.exp(z) + 1)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Shape of y_hat1: (60000, 16)\n",
            "Updated weights shape: (784, 16) (16, 10)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Q4.\n",
        "# making a class for adding two hidden layers\n",
        "class NeuralNetwork ():\n",
        "\n",
        "  def __init__ (self,x, softmax, sigmoid):\n",
        "    self.x = x\n",
        "    self.softmax = softmax\n",
        "    self.sigmoid = sigmoid\n",
        "    #self.relu = relu\n",
        "    #self.tanh = tanh\n",
        "    #self.sigmoid_deri = sigmoid_deri\n",
        "    self.weight1 = np.random.randn(784, 16)\n",
        "    self.bias1 = np.random.rand(1, 16)\n",
        "    self.weight2 = np.random.randn(16, 16)\n",
        "    self.bias2 = np.random.rand(1, 16)\n",
        "    self.weight3 = np.random.randn(16, 10)\n",
        "    self.bias3 = np.random.rand(1, 10)\n",
        "    self.activation_2 = self.sigmoid\n",
        "    self.activation_3 = self.softmax\n",
        "\n",
        "  def forward_prop (self,x):\n",
        "    self.x = x\n",
        "    self.z1 = np.dot(self.x, self.weight1) + self.bias1\n",
        "    self.y_hat1 = self.activation_2(self.z1)\n",
        "    self.z2 = np.dot(self.y_hat1, self.weight2) + self.bias2\n",
        "    self.y_hat2 = self.activation_2(self.z2) #self.softmax(self.z2)\n",
        "    self.z3 = np.dot(self.y_hat2, self.weight3) + self.bias3\n",
        "    self.y_hat3 = self.activation_3(self.z3)\n",
        "    print(\"Shape of y_hat2:\", self.y_hat2.shape)\n",
        "    return self.y_hat3\n",
        "\n",
        "  def back_prop (self, y, learn_rate):\n",
        "    #gradients for weights\n",
        "    self.deri_weight3 = np.sum(np.matmul(self.z3.T, (self.y_hat3 - y.T)))/60000\n",
        "    #self.deri_bias2 = np.mean(self.y_hat2 - y.T, axis=0, keepdims=True)\n",
        "    dz1 = (self.y_hat3 - y.T)\n",
        "    self.deri_weight2 = np.sum(np.matmul(self.z2.T, dz1))/60000\n",
        "    self.deri_weight1 = np.sum(np.matmul(self.x.T, dz1))/60000\n",
        "    #self.dervative1 = np.mean(self.sigmas - y.T, axis=0, keepdims=True)\n",
        "    #updating the weights and the bias\n",
        "    self.weight1 = self.weight1 - (learn_rate*self.deri_weight1)/60000\n",
        "    #self.bias1 = self.bias1 - (learn_rate*self.derivative2)/60000\n",
        "    self.weight2 = self.weight2 - (learn_rate*self.deri_weight2.T)/60000\n",
        "    #self.bias2 = self.bias2 - (learn_rate*self.derivative2)/60000\n",
        "    self.weight3 = self.weight3 - (learn_rate*self.deri_weight3.T)/60000\n",
        "    #self.bias3 = self.bias3  (learn_rate*self.derivative2)/60000\n",
        "    return self.weight1, self.weight2, self.weight3\n",
        "\n",
        "\n",
        "model3 = NeuralNetwork(x, model1.softmax, model2.sigmoid)\n",
        "learn_rate = 0.001\n",
        "#forward prop\n",
        "y_hat3 = model3.forward_prop(x)\n",
        "\n",
        "#back prop\n",
        "updated_weights1, weight2, weight3 = model3.back_prop(y, learn_rate)\n",
        "\n",
        "# printing for debugging\n",
        "print(\"Updated weights shape:\", updated_weights1.shape, weight2.shape, weight3.shape)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GU2E3z6YoiqR",
        "outputId": "f8842a28-66f2-456d-9b9e-ee8c38ee357a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-110-340b3e954109>:50: RuntimeWarning: overflow encountered in exp\n",
            "  sigma = 1 / (np.exp(z) + 1)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Shape of y_hat2: (60000, 16)\n",
            "Updated weights shape: (784, 16) (16, 16) (16, 10)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#we can write this class with the activation functions defined as methods\n",
        "#then we can pass the instances of this class to the other classes\n",
        "'''class ActivationFunction ():\n",
        "\n",
        " def __init__ (self,x,z):\n",
        "  self.x = x\n",
        "  self.z = z\n",
        " def relu(self,z):\n",
        "  delta = np.maximum(0,self.x)\n",
        "  return delta\n",
        "\n",
        " def softmax(self,z):\n",
        "    max_z = np.max(self.z, axis=1, keepdims=True)\n",
        "    #<ipython-input-28-9f6efb757d7b>:23: RuntimeWarning: overflow encountered in exp\n",
        "    #therefore using the max to subtract from logits of z, log-sum-exp approach\n",
        "    #Reference : https://gregorygundersen.com/blog/2020/02/09/log-sum-exp/\n",
        "    e = np.exp(self.z)\n",
        "    e = np.exp(self.z - max_z)\n",
        "    e_sum = np.sum(e, axis = 1, keepdims = True)\n",
        "    sigma = e/e_sum\n",
        "    return sigma\n",
        "\n",
        " def sigmoid (self, z):\n",
        "    self.z = np.dot(self.x, self.weight1) + self.bias1\n",
        "    sigma = 1 / (np.exp(-z) + 1)\n",
        "    return sigma\n",
        "\n",
        " def tanh (self):\n",
        "    f = (2/(np.exp(-2 * self.x) + 1)) - 1\n",
        "    return f\n",
        "\n",
        "activation_func = ActivationFunction(x,z)'''"
      ],
      "metadata": {
        "id": "XOYDxCdMjwrX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Q5.\n",
        "class NNetwork ():\n",
        "#initializing the weights and the bias\n",
        "#then making the instances of the activation functions\n",
        " def __init__ (self, x, softmax, sigmoid, back_prop):\n",
        "  self.x = x\n",
        "  self.softmax = softmax\n",
        "  self.sigmoid = sigmoid\n",
        "  self.back_prop = back_prop\n",
        "  self.weight1 = np.random.randn(784, 16)\n",
        "  self.bias1 = np.random.rand(1, 16)\n",
        "  self.weight2 = np.random.randn(16, 16)\n",
        "  self.bias2 = np.random.rand(1, 16)\n",
        "  self.weight3 = np.random.randn(16, 10)\n",
        "  self.bias3 = np.random.rand(1, 10)\n",
        "  self.activation_2 = self.sigmoid\n",
        "  self.activation_3 = self.softmax\n",
        "  self.activation_4 = self.relu\n",
        "  self.activation_5 = self.tanh\n",
        "  #self.sigmoid_deri = sigmoid_deri\n",
        "#doing forward propagation\n",
        " def forward_prop(self):\n",
        "    self.x = x\n",
        "    self.z1 = np.dot(self.x, self.weight1) + self.bias1\n",
        "    self.y_hat1 = self.activation_2(self.z1) #sigmoid\n",
        "    self.z2 = np.dot(self.y_hat1, self.weight2) + self.bias2\n",
        "    self.y_hat2 = self.activation_4(self.z2) #ReLU\n",
        "    self.z3 = np.dot(self.y_hat2, self.weight3) + self.bias3\n",
        "    self.y_hat3 = self.activation_3(self.z3) #softmax for the output\n",
        "    print(\"Shape of y_hat2:\", self.y_hat2.shape)\n",
        "    return self.y_hat3\n",
        "#using ReLU activation function\n",
        " def relu(self,z):\n",
        "  delta = np.maximum(0,self.x)\n",
        "  return delta\n",
        " def tanh (self):\n",
        "    f = (2/(np.exp(-2 * self.x) + 1)) - 1\n",
        "    return f\n",
        "\n",
        "model4 = NNetwork (x, model1.softmax, model2.sigmoid, model3.back_prop)\n",
        "\n",
        "updated_weights1, weight2, weight3 = model4.back_prop(y, learn_rate)\n",
        "\n",
        "# printing for debugging\n",
        "print(\"Updated weights shape:\", updated_weights1.shape, weight2.shape, weight3.shape)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "E-yTzVn-hW7G",
        "outputId": "38fb53c7-47d6-41e7-aa8c-ad054f17d2c1"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Updated weights shape: (784, 16) (16, 16) (16, 10)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Q6.\n",
        "class MomentumGD (NNetwork):\n",
        "\n",
        "  def __init__ (self, x, epoch):\n",
        "     super().__init__(x, model1.softmax, model2.sigmoid, model3.back_prop)\n",
        "     self.x = x\n",
        "     self.epoch = epoch\n",
        "     #initialzing the velocities\n",
        "     self.v1 = 0\n",
        "     self.v2 = 0\n",
        "     self.v3 = 0\n",
        "#we can define this method for momentum\n",
        "  '''def momentum (self, beta, learn_rate):\n",
        "    self.v1 = self.v1 * beta - (1-beta)*self.gradw1\n",
        "    self.v2 = self.v2 * beta - (1-beta)*self.gradw2\n",
        "    self.v3 = self.v3 * beta - (1-beta)*self.gradw3\n",
        "    self.weight1 = self.weight1 - (learn_rate * self.v1)\n",
        "    self.weight2 = self.weight2 - (learn_rate * self.v2)\n",
        "    self.weight3 = self.weight3 - (learn_rate * self.v3)\n",
        "    return'''\n",
        "\n",
        "  def back_prop1(self, y):\n",
        "    self.gradw3 = np.sum(np.matmul(self.z3.T, (self.y_hat3 - y.T)))/60000\n",
        "    #self.deri_bias2 = np.mean(self.y_hat2 - y.T, axis=0, keepdims=True)\n",
        "    dz1 = (self.y_hat3 - y.T)\n",
        "    self.gradw2 = np.sum(np.matmul(self.z2.T, dz1))/60000\n",
        "    self.gradw1 = np.sum(np.matmul(self.x.T, dz1))/60000\n",
        "    #self.dervative1 = np.mean(self.sigmas - y.T, axis=0, keepdims=True)\n",
        "    #self.weight1 = self.weight1 - (learn_rate*self.deri_weight1)/60000\n",
        "    #self.bias1 = self.bias1 - (learn_rate*self.derivative2)/60000\n",
        "    #self.weight2 = self.weight2 - (learn_rate*self.deri_weight2.T)/60000\n",
        "    #self.bias2 = self.bias2 - (learn_rate*self.derivative2)/60000\n",
        "    #self.weight3 = self.weight3 - (learn_rate*self.deri_weight3.T)/60000\n",
        "    #self.bias3 = self.bias3  (learn_rate*self.derivative2)/60000\n",
        "    return self.gradw1, self.gradw2, self.gradw3\n",
        "\n",
        "\n",
        "  def train (self,beta):\n",
        "    for i in range(epoch):\n",
        "#forward prop\n",
        "        self.x = x\n",
        "        self.z1 = np.dot(self.x, self.weight1) + self.bias1\n",
        "        self.y_hat1 = self.activation_2(self.z1)\n",
        "        self.z2 = np.dot(self.y_hat1, self.weight2) + self.bias2\n",
        "        self.y_hat2 = self.activation_2(self.z2) #self.softmax(self.z2)\n",
        "        self.z3 = np.dot(self.y_hat2, self.weight3) + self.bias3\n",
        "        self.y_hat3 = self.activation_3(self.z3)\n",
        "# back prop\n",
        "        self.gradw1, self.gradw2, self.gradw3 = self.back_prop1(y)\n",
        "# momentum update\n",
        "        #self.momentum(beta, learn_rate)\n",
        "        self.v1 = self.v1 * beta - (1-beta)*self.gradw1\n",
        "        self.v2 = self.v2 * beta - (1-beta)*self.gradw2\n",
        "        self.v3 = self.v3 * beta - (1-beta)*self.gradw3\n",
        "        '''self.vbias1 = self.v1 * beta - (1-beta) *self.gradb1'''\n",
        "        #updating the weights\n",
        "        self.weight1 = self.weight1 - (learn_rate * self.v1)\n",
        "        self.weight2 = self.weight2 - (learn_rate * self.v2)\n",
        "        self.weight3 = self.weight3 - (learn_rate * self.v3)\n",
        "        '''self.bias1 = self.bias1 - (learn_rate * self.vbias1)'''\n",
        "        pass\n",
        "\n",
        "    return self.weight1, self.weight2, self.weight3\n",
        "#beta is the momentum hyperparameter\n",
        "beta = 0.9\n",
        "epoch = 10\n",
        "model5 = MomentumGD(x, epoch)\n",
        "updated_weights1, weight2, weight3 = model5.train(beta)\n",
        "\n",
        "#printing for debugging\n",
        "print(\"Updated weights shape:\", updated_weights.shape, weight2.shape, weight3.shape)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "m7IQ0GQXrR8p",
        "outputId": "85250a6b-2649-4306-cfca-49bc819a9fd5"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-110-340b3e954109>:50: RuntimeWarning: overflow encountered in exp\n",
            "  sigma = 1 / (np.exp(z) + 1)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Updated weights shape: (784, 10) (16, 16) (16, 10)\n"
          ]
        }
      ]
    }
  ]
}