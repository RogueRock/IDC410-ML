{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyPNdiBGQ5rqpOSKYHMt622o",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/RogueRock/IDC410-ML/blob/main/ML_exercise2_logistic_regression.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ADj0KOsQT2Az"
      },
      "outputs": [],
      "source": [
        "\"\"\"generate an m+1 dimensional data set, of size n, consisting of m continuous independent\n",
        "variables (X) and one dependent binary variable (Y) \"\"\"\n",
        "import numpy as np\n",
        "def dataset_generator (n,m, theta):\n",
        "  X = np.random.rand(n,m)\n",
        "  #to get 1 in the first column as the bias\n",
        "  X_bias = np.column_stack([np.ones(n), X])\n",
        "  #the coefficients of the linear relationship of X and Y\n",
        "  beta = np.random.rand(m+1)\n",
        "  #calculating dot product of X_bias and beta\n",
        "  X_product = np.dot(X_bias, beta)\n",
        "  prob = 1/(1+np.exp(-X_product))\n",
        "  #assuming Bernoulli distribution with a probability of success theta to determine flipping\n",
        "  flip = np.random.binomial(1, theta, size = n)\n",
        "  Y = np.zeros(n)\n",
        "  for i in range (n):\n",
        "     if flip[i] == 1:\n",
        "      Y[i] = 1 - prob[i]\n",
        "     else:\n",
        "      Y[i] = prob[i]\n",
        "  Y = (Y > 0.5).astype(int)\n",
        "  return X_bias, beta , Y\n",
        "n = 50\n",
        "m = 3\n",
        "theta = 0.5\n",
        "X_bias, beta, Y = dataset_generator (n,m,theta)\n",
        "print (\"The independent variable (X): \" )\n",
        "print (X_bias)\n",
        "print (\"-\" * 45)\n",
        "print (\"The random coefficients (beta) : \",beta)\n",
        "print(\"The dependent variable (Y): \" , Y)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\" Function to learn the inputs of the logistic regression\n",
        "and implementing the gradient descent algotrithm for the cost function \"\"\"\n",
        "#Z is what we get after dot product of X_i with beta_j\n",
        "def sigmoid (Z):\n",
        "  return 1/(1+np.exp(-Z))\n",
        "#the cost function log loss\n",
        "def log_loss (Y_hat,Y):\n",
        "  return -np.mean((Y * np.log(Y_hat) + (1-Y) * np.log(1-Y_hat)))\n",
        "def gradient_descent (X, Y, learn_rate, tau, epoch):\n",
        "  beta_initialized = np.random.rand(m+1)\n",
        "  print(\"initial betas : \" , beta_initialized)\n",
        "  error = []\n",
        "  prev_cost = np.inf\n",
        "  cost = np.zeros(epoch)\n",
        "  for i in range (epoch):\n",
        "    Z = np.dot(X, beta_initialized)\n",
        "    Y_hat = sigmoid(Z)\n",
        "    cost[i] = log_loss(Y_hat,Y)\n",
        "    derivative = np.dot(X.T, (Y_hat-Y))/(Y.size)\n",
        "    beta_initialized = beta_initialized - learn_rate * derivative\n",
        "    error.append(cost[i])\n",
        "    if epoch > 0 and np.abs(cost[i] - cost[i - 1]) < tau:\n",
        "       break\n",
        "    final_cost = log_loss(Y_hat,Y)\n",
        "    return  beta_initialized, final_cost\n",
        "n = 50\n",
        "m = 3\n",
        "theta = 0.9\n",
        "X_bias, beta_original, Y = dataset_generator(n,m,theta)\n",
        "learn_rate = 0.001\n",
        "tau = 1*10**-6\n",
        "epoch = 1000\n",
        "learned_beta , final_cost = gradient_descent(X_bias, Y, learn_rate, tau, epoch)\n",
        "print(\"Final betas : \" , learned_beta)\n",
        "print(\"The final cost : \", final_cost)\n"
      ],
      "metadata": {
        "id": "fV0fA1CNMmnc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#adding L1 and L2 regularization to the cost funtion\n",
        "def l1_regularization (Y_hat, Y, penalty_l1, params):\n",
        "  return -np.mean(Y * np.log(Y_hat) + (1-Y) * np.log(1-Y_hat)) + penalty_l1 * np.mean(np.abs(params))\n",
        "def l2_regularization (Y_hat, Y, penalty_l2, params):\n",
        "  return -np.mean(Y * np.log(Y_hat) + (1-Y) * np.log(1-Y_hat)) + penalty_l2 * np.mean(np.square(params))\n",
        "#initializing gradient descent with L1 regularization\n",
        "def gradient_descent_l1 (X, Y ,params, learn_rate,tau, epoch):\n",
        "  error = []\n",
        "  prev_cost = np.inf\n",
        "  cost = np.zeros(epoch)\n",
        "  for i in range (epoch):\n",
        "    Z = np.dot(X, params)\n",
        "    Y_hat = sigmoid(Z)\n",
        "    cost[i] = l1_regularization(Y_hat,Y, penalty_l1, params)\n",
        "    derivative = np.dot(X.T, (Y_hat-Y))/(Y.size) + penalty_l1 * np.sign(params)\n",
        "    params = params - learn_rate * derivative\n",
        "    error.append(cost[i])\n",
        "    if epoch > 0 and np.abs(cost[i] - cost[i - 1]) < tau:\n",
        "       break\n",
        "    final_cost_l1 = l1_regularization(Y_hat, Y, penalty_l1, params)\n",
        "    return  params, final_cost_l1\n",
        "#initializing gradient descent with L1 regularization\n",
        "def gradient_descent_l2 (X, Y, params, learn_rate, tau, epoch):\n",
        "  error = []\n",
        "  prev_cost = np.inf\n",
        "  cost = np.zeros(epoch)\n",
        "  for i in range (epoch):\n",
        "    Z = np.dot(X, params)\n",
        "    Y_hat = sigmoid(Z)\n",
        "    cost[i] = l2_regularization(Y_hat,Y, penalty_l2, params)\n",
        "    derivative = np.dot(X.T, (Y_hat-Y))/Y.size + penalty_l2 * 2 * params\n",
        "    params = params - learn_rate * derivative\n",
        "    error.append(cost[i])\n",
        "    if epoch > 0 and np.abs(cost[i] - cost[i - 1]) < tau:\n",
        "       break\n",
        "    final_cost_l2 = l2_regularization(Y_hat, Y, penalty_l2, params)\n",
        "    return  params, final_cost_l2\n",
        "params = np.random.randn(m+1)\n",
        "penalty_l1 = 2.12\n",
        "penalty_l2 = 1\n",
        "learned_params_l1 , final_cost_l1 = gradient_descent_l1 (X_bias, Y, params, learn_rate, tau, epoch)\n",
        "learned_params_l2 , final_cost_l2 = gradient_descent_l2 (X_bias, Y, params, learn_rate, tau, epoch)\n",
        "print (\"Learned beta values (L1 regularization) : \" ,learned_params_l1)\n",
        "print (final_cost_l1)\n",
        "print (\"Learned beta values (L2 regularization) : \" ,learned_params_l2)\n",
        "print (final_cost_l2)\n"
      ],
      "metadata": {
        "id": "XdbY-8LRPkT9"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}